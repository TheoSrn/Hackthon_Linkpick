# SystÃ¨me RAG pour Offres d'Emploi France Travail

Un systÃ¨me complet de Retrieval-Augmented Generation (RAG) permettant d'analyser les CV et de rechercher des offres d'emploi correspondantes sur France Travail en temps rÃ©el.

## ðŸŽ¯ FonctionnalitÃ©s

- **ðŸ“„ Analyse de CV intelligente** : Upload de CV (PDF/DOCX) avec extraction automatique du profil
- **ðŸ” Recherche en temps rÃ©el** : RequÃªtes directes sur l'API France Travail (pas de base prÃ©-indexÃ©e)
- **ðŸ¤– Correspondance IA** : Analyse de compatibilitÃ© CV-offres avec recommandations personnalisÃ©es
- **ðŸ“Š StratÃ©gies de recherche multiples** : Fallback automatique du plus spÃ©cifique au plus gÃ©nÃ©ral
- **ðŸ’¡ GÃ©nÃ©ration de rÃ©ponses contextuelles** avec vLLM et Qwen2.5-1.5B-Instruct
- **ðŸ“ RÃ©ponses formatÃ©es en Markdown** (listes, gras, structure claire)
- **ðŸŒ Interface web franÃ§aise** intuitive et Ã©purÃ©e
- **ðŸ‹ DÃ©ploiement facile** avec Docker Compose
- **âš¡ AccÃ©lÃ©ration GPU** avec support NVIDIA CUDA

## ðŸ“‹ PrÃ©requis

- Docker et Docker Compose installÃ©s
- **GPU NVIDIA** (recommandÃ©, 8GB VRAM minimum) avec drivers CUDA
  - TestÃ© sur RTX 4060 Laptop (8GB VRAM)
- Au moins 16 GB de RAM systÃ¨me
- **Credentials API France Travail** (obligatoire - voir section Configuration)

## âš™ï¸ Configuration de l'API France Travail

### 1. Obtenir vos credentials

1. Rendez-vous sur [France Travail Connect](https://francetravail.io/)
2. CrÃ©ez un compte dÃ©veloppeur
3. Inscrivez-vous Ã  l'API "Offres d'emploi v2"
4. RÃ©cupÃ©rez votre `client_id` et `client_secret`

### 2. Configurer les variables d'environnement

Copiez le fichier `.env.example` en `.env` :

```bash
cp .env.example .env
```

Ã‰ditez le fichier `.env` et ajoutez vos credentials :

```env
FRANCE_TRAVAIL_CLIENT_ID=votre_client_id
FRANCE_TRAVAIL_CLIENT_SECRET=votre_client_secret
MAX_JOB_OFFERS=500
```

## ðŸš€ Installation et DÃ©marrage

### 1. Lancer le systÃ¨me

```bash
# DÃ©marrer tous les services
docker compose up -d

# VÃ©rifier le statut
docker compose ps
```

Le systÃ¨me va :
1. **DÃ©marrer Qdrant** (base de donnÃ©es vectorielle - optionnel pour la recherche classique)
2. **Lancer l'indexeur** pour rÃ©cupÃ©rer et indexer les offres France Travail (optionnel)
   - L'indexeur vÃ©rifie si la collection existe et contient dÃ©jÃ  des donnÃ©es
   - Si oui, il quitte immÃ©diatement (pas de rÃ©indexation)
   - Si non, il rÃ©cupÃ¨re jusqu'Ã  500 offres et les indexe
3. **DÃ©marrer vLLM** (serveur d'infÃ©rence avec Qwen2.5-1.5B-Instruct)
   - Attend que l'indexeur termine
   - Charge le modÃ¨le sur GPU
4. **Lancer l'API backend** FastAPI
5. **Lancer l'interface web** Nginx

**Note :** Le premier dÃ©marrage peut prendre 10-15 minutes (tÃ©lÃ©chargement du modÃ¨le Qwen ~3GB et initialisation GPU).

### 3. AccÃ©der au systÃ¨me

- **Interface Web** : http://localhost
- **API Backend** : http://localhost:8001
- **API Documentation** : http://localhost:8001/docs
- **Dashboard Qdrant** : http://localhost:6333/dashboard

## ðŸ’¼ Utilisation - Upload de CV

### Via l'Interface Web

1. Ouvrez http://localhost dans votre navigateur
2. Cliquez sur l'onglet **"Upload CV"**
3. SÃ©lectionnez votre CV (PDF ou DOCX)
4. Le systÃ¨me va :
   - **Extraire le texte** de votre CV
   - **Analyser votre profil** (compÃ©tences, expÃ©rience, mÃ©tier recherchÃ©)
   - **Rechercher sur France Travail** les offres correspondantes en temps rÃ©el
   - **GÃ©nÃ©rer une analyse dÃ©taillÃ©e** de compatibilitÃ©
5. Consultez les rÃ©sultats :
   - **RÃ©sumÃ© de profil** : SynthÃ¨se de vos compÃ©tences
   - **Offres recommandÃ©es** : Liste des emplois correspondants
   - **Analyse IA** : Pourquoi chaque offre correspond Ã  votre profil
   - **Conseils personnalisÃ©s** : Recommandations pour optimiser vos candidatures

### Via l'API (cURL)

```bash
curl -X POST http://localhost:8001/upload-cv?top_k=10 \
  -F "file=@mon_cv.pdf"
```

**RÃ©ponse :**
```json
{
  "profile_summary": "DÃ©veloppeur Full Stack avec 5 ans d'expÃ©rience...",
  "matching_offers": [
    {
      "intitule": "DÃ©veloppeur Full Stack H/F",
      "entreprise": "TechCorp",
      "lieu": "Paris 15e Arrondissement (75)",
      "type_contrat": "CDI",
      "url_postuler": "https://...",
      "date_creation": "2026-02-03T10:30:00.000Z",
      "score": 1.0,
      "description": "Nous recherchons un dÃ©veloppeur Full Stack..."
    }
  ],
  "analysis": "## Correspondance gÃ©nÃ©rale\n\nVotre profil..."
}
```

## ðŸ” Exemple d'Utilisation - Recherche Classique

### Via l'Interface Web

1. Ouvrez http://localhost dans votre navigateur
2. Attendez que le statut systÃ¨me indique "âœ“ SystÃ¨me prÃªt"
3. Tapez votre question en franÃ§ais dans le champ de texte
4. Cliquez sur "Rechercher"
5. Consultez la rÃ©ponse formatÃ©e en Markdown

**Exemples de questions :**
```
Quelles sont les offres de dÃ©veloppeur Python disponibles ?

Liste-moi les offres en CDI dans le secteur tech

Trouve-moi des postes de Data Scientist Ã  Paris
```

### Via l'API (cURL)

**Recherche avec gÃ©nÃ©ration de rÃ©ponse :**

```bash
curl -X POST http://localhost:8001/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Quelles sont les offres de dÃ©veloppeur disponibles ?",
    "top_k": 3
  }'
```

**RÃ©ponse :**
```json
{
  "answer": "Voici les offres de dÃ©veloppeur trouvÃ©es :\n\n1. **DÃ©veloppeur Full Stack** chez TechCorp\n...",
  "sources": [
    {
      "intitule": "DÃ©veloppeur Full Stack",
      "entreprise": "TechCorp",
      "chunk_id": 5,
      "score": 0.89,
      "text": "Extrait du texte de l'offre..."
    }
  ]
}
```

### Via l'API (Python)

```python
import requests

response = requests.post(
    "http://localhost:8001/query",
    json={
        "question": "Liste les offres de Data Scientist",
        "top_k": 3
    }
)

result = response.json()
print("RÃ©ponse:", result["answer"])
print("\nSources:")
for source in result["sources"]:
    print(f"- {source['intitule']} chez {source['entreprise']} (Score: {source['score']:.2f})")
```

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚  â† Interface web franÃ§aise (Nginx + Upload CV)
â”‚ (Port 80)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Backend   â”‚  â† API FastAPI modulaire
â”‚ (Port 8001) â”‚     â€¢ services/cv_service.py
â”‚             â”‚     â€¢ services/france_travail_service.py
â”‚             â”‚     â€¢ services/llm_service.py
â”‚             â”‚     â€¢ services/qdrant_service.py (optionnel)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
    â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”  â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Qdrant â”‚  â”‚   vLLM    â”‚  â”‚ France Travail â”‚
â”‚ (6333) â”‚  â”‚  (8000)   â”‚  â”‚      API       â”‚
â”‚        â”‚  â”‚  Qwen2.5  â”‚  â”‚  (temps rÃ©el)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ðŸ”„ Flux de traitement CV

1. **Upload** â†’ Extraction texte (PDF/DOCX)
2. **Analyse LLM** â†’ Extraction profil + mots-clÃ©s
3. **Recherche API** â†’ France Travail (stratÃ©gies multiples)
4. **Analyse IA** â†’ Correspondance CV-offres
5. **RÃ©ponse** â†’ Offres + recommandations

## ðŸ“ Structure du Code Backend

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ config.py              # Configuration centralisÃ©e
â”‚   â”œâ”€â”€ main.py                # Routes FastAPI principales
â”‚   â”œâ”€â”€ models.py              # ModÃ¨les Pydantic
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ cv_service.py            # Traitement des CV
â”‚   â”‚   â”œâ”€â”€ france_travail_service.py # IntÃ©gration API France Travail
â”‚   â”‚   â”œâ”€â”€ llm_service.py           # Client vLLM
â”‚   â”‚   â””â”€â”€ qdrant_service.py        # Client Qdrant (optionnel)
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ file_extractors.py       # Extraction PDF/DOCX
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt
```

## ðŸ”§ SpÃ©cifications Techniques

### ModÃ¨les et Embeddings
- **LLM** : Qwen/Qwen2.5-1.5B-Instruct (1.5B paramÃ¨tres)
  - Context window : 4096 tokens
  - Format : float16
  - GPU memory utilization : 70%
- **Embeddings** : sentence-transformers/all-MiniLM-L6-v2
  - Dimension : 384
  - UtilisÃ© pour la recherche sÃ©mantique

### ðŸŽ¯ StratÃ©gies de Recherche France Travail

Le systÃ¨me utilise des stratÃ©gies de recherche progressives pour maximiser les rÃ©sultats :

1. **StratÃ©gie 1** : MÃ©tier + premiÃ¨res compÃ©tences (ex: "DÃ©veloppeur Python Machine Learning")
2. **StratÃ©gie 2** : CompÃ©tences restantes (si > 2 compÃ©tences)
3. **StratÃ©gie 3** : MÃ©tier uniquement (ex: "DÃ©veloppeur Python")
4. **StratÃ©gie 4** : PremiÃ¨re compÃ©tence uniquement (ex: "Python")
5. **StratÃ©gie 5** : Sans filtres (offres rÃ©centes gÃ©nÃ©rales)

Chaque stratÃ©gie est essayÃ©e sÃ©quentiellement jusqu'Ã  obtenir des rÃ©sultats.

### ðŸ—„ï¸ Base de DonnÃ©es (Optionnel)
- **Qdrant** : Base vectorielle pour stocker les offres
- **Collection** : "job_offers"
- **Note** : La fonctionnalitÃ© principale (Upload CV) utilise l'API France Travail directement

### âš™ï¸ Configuration GPU
- CUDA graphs activÃ©s
- FlashAttention backend
- Memory utilization : ~7.5GB sur 8GB VRAM

## Configuration

### Modifier le nombre de rÃ©sultats

Dans `backend/main.py` :
```python
TOP_K = 3  # Nombre d'offres Ã  rÃ©cupÃ©rer pour le contexte
```

### Modifier le nombre d'offres indexÃ©es

Dans le fichier `.env` :
```env
MAX_JOB_OFFERS=500  # Nombre maximum d'offres Ã  indexer
```

### Utiliser un modÃ¨le diffÃ©rent

Dans `docker-compose.yml`, section vLLM :
```yaml
vllm:
  command: >
    --model Qwen/Qwen2.5-1.5B-Instruct
    --max-model-len 4096
    --dtype float16
    --gpu-memory-utilization 0.7
```

ModÃ¨les compatibles recommandÃ©s :
- `Qwen/Qwen2.5-1.5B-Instruct` (actuel, 4096 tokens)
- `Qwen/Qwen2.5-3B-Instruct` (plus performant, nÃ©cessite plus de VRAM)
- `microsoft/Phi-3-mini-4k-instruct` (alternatif)

### Ajuster la longueur du contexte

Modifier `--max-model-len` dans docker-compose.yml :
```yaml
--max-model-len 4096  # Peut aller jusqu'Ã  32768 pour Qwen2.5
```

## Statistiques du SystÃ¨me

Consultez les statistiques d'indexation :

```bash
curl http://localhost:8001/stats
```

RÃ©ponse :
```json
{
  "collection_name": "job_offers",
  "total_chunks": 500,
  "vector_size": 384
}
```

## Commandes Utiles

```bash
# Voir les logs en temps rÃ©el
docker compose logs -f

# Logs d'un service spÃ©cifique
docker compose logs -f backend
docker compose logs -f vllm

# RedÃ©marrer un service
docker compose restart backend

# ArrÃªter le systÃ¨me
docker compose down

# ArrÃªter et supprimer les volumes (rÃ©initialisation complÃ¨te)
docker compose down -v

# Re-indexer les offres (supprime et recrÃ©e la collection)
docker compose down qdrant indexer
docker compose up -d qdrant indexer

# VÃ©rifier l'Ã©tat des conteneurs
docker compose ps

# VÃ©rifier l'utilisation GPU
nvidia-smi

# Rebuild un service aprÃ¨s modification
docker compose up -d --build backend
```

## DÃ©pannage

### Le systÃ¨me ne dÃ©marre pas

VÃ©rifiez que les ports ne sont pas dÃ©jÃ  utilisÃ©s :
```bash
lsof -i :80,8001,6333,8000
```

### Pas de rÃ©sultats trouvÃ©s

VÃ©rifiez que les offres sont indexÃ©es :
```bash
curl http://localhost:8001/stats
# Doit afficher total_chunks > 0
```

Si vide, rÃ©indexer :
```bash
docker compose restart indexer
docker compose logs -f indexer
```

### vLLM ne dÃ©marre pas / Out of Memory

Le modÃ¨le Qwen2.5-1.5B nÃ©cessite ~7.5GB VRAM. Solutions :

1. **RÃ©duire la mÃ©moire utilisÃ©e** (docker-compose.yml) :
```yaml
--gpu-memory-utilization 0.6  # au lieu de 0.7
```

2. **RÃ©duire le contexte** :
```yaml
--max-model-len 2048  # au lieu de 4096
```

3. **VÃ©rifier CUDA** :
```bash
nvidia-smi  # VÃ©rifier GPU disponible
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### Les rÃ©ponses sont lentes

- **Normal** : PremiÃ¨re requÃªte prend ~30s (warmup)
- **AprÃ¨s** : ~5-10s par requÃªte sur RTX 4060 Laptop
- Pour accÃ©lÃ©rer : Utiliser GPU plus puissant ou modÃ¨le plus petit

### L'interface ne charge pas

VÃ©rifier que tous les services sont UP :
```bash
docker compose ps
# Tous doivent Ãªtre "running" et "healthy"
```

VÃ©rifier les logs du backend :
```bash
docker compose logs backend
```

### Credentials France Travail invalides

Erreur : `401 Unauthorized` dans les logs indexer

Solution :
1. VÃ©rifier `.env` avec les bons credentials
2. Redemander l'accÃ¨s sur [France Travail Connect](https://francetravail.io/)
3. VÃ©rifier que l'API "Offres d'emploi v2" est bien souscrite

### Le Markdown ne s'affiche pas

VÃ©rifier que `marked.js` est bien chargÃ© dans le frontend :
```bash
curl http://localhost/index.html | grep marked
# Doit trouver: <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js">
```

## Structure du Projet

```
Hackthon_Linkpick/
â”œâ”€â”€ indexer/              # Service d'indexation
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ index_documents.py
â”œâ”€â”€ backend/              # API FastAPI
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ frontend/             # Interface web
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ docker-compose.yml    # Configuration Docker
â””â”€â”€ README.md
```
